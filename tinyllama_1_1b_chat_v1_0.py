# -*- coding: utf-8 -*-
"""TinyLlama-1.1B-Chat-v1.0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dsPV4xlayW5qDzmqJWxj4lKJd1Uul8qp
"""

!pip install -q vllm

# 2. Import vLLM
from vllm import LLM, SamplingParams

# 3. Define prompts
prompts = [
    "Tell me about Python",
    "What is the attention mechanism? ",
]

# 4. Define sampling parameters
sampling_params = SamplingParams(
    temperature=0.87,
    top_p=0.90,
    max_tokens=60,  # number of tokens to generate
)

# 5. Run generation
llm = LLM(model="TinyLlama/TinyLlama-1.1B-Chat-v1.0")

outputs = llm.generate(prompts, sampling_params)

# 6. Print results
for i, output in enumerate(outputs):
    print(f"\nPrompt {i+1}: {prompts[i]}")
    print(f"Completion: {output.outputs[0].text}")